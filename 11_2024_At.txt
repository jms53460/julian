#November 2024 data includes 4 strips of tomato and 20 of Arabidopsis. These tomato samples were from the old tomato plant I took cuttings from (24S45-1)
#Downloading data from Duke
ssh jms53460@xfer.gacrc.uga.edu
cd /work/bnlab
mkdir Nov2024Seq
cd Nov2024Seq
sftp scroggs_10506@dnaseq2.igsp.duke.edu
#enter the password: CSqMHVIA4OHr
get -r Scroggs_10506_241203B9

cp A254-266_S2_L002_R* /home/jms53460/Nov2024Seq/
cp S1-8_A267-277_S3_L002_R* /home/jms53460/Nov2024Seq/

mkdir /scratch/jms53460/11_2024_At
mkdir /scratch/jms53460/11_2024_At/Raw_Data
cp A254-266_S2_L002_R* /scratch/jms53460/11_2024_At/Raw_Data
cp S1-8_A267-277_S3_L002_R* /scratch/jms53460/11_2024_At/Raw_Data

cp -r /home/jms53460/Ler_0_N-masked /scratch/jms53460/11_2024_At/
cp /home/jms53460/CELSeq_barcodes.txt /scratch/jms53460/11_2024_At/
cp /home/jms53460/Ler_SNPs.tab /scratch/jms53460/11_2024_At/
cp /home/jms53460/TAIR10.1_Col_5.gff /scratch/jms53460/11_2024_At/


#Backup raw data
scp sapelo2:/scratch/jms53460/11_2024_At/Raw_Data/*.fastq.gz 'D:\11_2024_At_Sl_Data'
scp sapelo2:/work/bnlab/Nov2024Seq/Scroggs_10506_241203B9/* 'D:\Nov2024Seq'


#Demultiplex data in a way that it is nearly ready for upload with SRA.

#!/bin/bash
#SBATCH --job-name=At_Sl_demultiplex                                         # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=6                                                 # Number of cores per task
#SBATCH --mem=50gb                                                       # Total memory for job
#SBATCH --time=6:00:00                                                   # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/11_2024_At/At_Sl_dm.out                  # Location of standard output file
#SBATCH --error=/scratch/jms53460/11_2024_At/At_Sl_dm.err                   # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=END,FAIL                                              # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/11_2024_At/
mkdir Demultiplexed
ml Miniconda3/23.5.2-0
source activate /home/jms53460/Fastq-Multx

for file in Raw_Data/*_R1_*.gz; do
    filename=$(basename "$file")
    file2=$(echo "$filename" | sed 's/_R1.*//' | sed 's/_R2_001.fastq.gz//')

    if [ ! -f "Demultiplexed/""$file2""_1s.fastq.gz" ]; then
        module load fastp/0.23.2-GCC-11.3.0
	    fastp -w 6 -i "$file" -I "Raw_Data/""$file2""_R2_001.fastq.gz" -o "Demultiplexed/umi_""$file2""_R1.fastq.gz" -O "Demultiplexed/umi_""$file2""_R2.fastq.gz" -A -Q -L -G --umi --umi_loc read2 --umi_len 10 --umi_prefix UMI

	    fastq-multx -b -B "CELSeq_barcodes.txt" -m 0 "Demultiplexed/umi_""$file2""_R2.fastq.gz" "Demultiplexed/umi_""$file2""_R1.fastq.gz" "Raw_Data/""$file2""_R2_001.fastq.gz" -o "Demultiplexed/""$file2""_%_R2.fastq.gz" "Demultiplexed/""$file2""_%.fastq.gz" "Demultiplexed/""$file2""_%_umi.fastq.gz" # Split read 2 file by CELseq barcodes. Require perfect match to barcode in expected location

    fi
done
conda deactivate


###nohup: leaves the cluster doing the thing when you leave the wifi




##
#Trying to similuate losing the header and then trying to run fastp to readd UMI info.
cp Demultiplexed3/A254-266_S2_L002_1s.fastq.gz .
cp Demultiplexed3/A254-266_S2_L002_1s_R2.fastq.gz .
zcat A254-266_S2_L002_1s.fastq.gz | sed 's/UMI_.* / /' | head
zcat A254-266_S2_L002_1s.fastq.gz | sed 's/@.*/@/' > A254-266_S2_L002_1s_stripped.fastq
zcat A254-266_S2_L002_1s_R2.fastq.gz | sed 's/@.*/@/' > A254-266_S2_L002_1s_R2_stripped.fastq
gzip A254-266_S2_L002_1s_stripped.fastq
gzip A254-266_S2_L002_1s_R2_stripped.fastq
fastp -w 6 -i A254-266_S2_L002_1s_stripped.fastq.gz -I A254-266_S2_L002_1s_R2_stripped.fastq.gz -o A254-266_S2_L002_1s_umi.fastq.gz -O A254-266_S2_L002_1s_R2_umi.fastq.gz -A -Q -L -G --umi --umi_loc read2 --umi_len 10 --umi_prefix UMI
#fastp deletes the UMIs from the read sequence when it moves them to the header
##

#Move the tomato data to a different directory
mkdir /scratch/jms53460/11_2024_Sl
mkdir /scratch/jms53460/11_2024_Sl/Demultiplexed
cd Demultiplexed
mv S1-8_A267-277*_1* /scratch/jms53460/11_2024_Sl/Demultiplexed
mv S1-8_A267-277*_2* /scratch/jms53460/11_2024_Sl/Demultiplexed
mv S1-8_A267-277*_3s* /scratch/jms53460/11_2024_Sl/Demultiplexed
mv S1-8_A267-277*_4s* /scratch/jms53460/11_2024_Sl/Demultiplexed
mv S1-8_A267-277*_5s* /scratch/jms53460/11_2024_Sl/Demultiplexed
mv S1-8_A267-277*_6s* /scratch/jms53460/11_2024_Sl/Demultiplexed
mv S1-8_A267-277*_7s* /scratch/jms53460/11_2024_Sl/Demultiplexed
mv S1-8_A267-277*_8s* /scratch/jms53460/11_2024_Sl/Demultiplexed
mv S1-8_A267-277*_9s* /scratch/jms53460/11_2024_Sl/Demultiplexed
mv S1-8_A267-277*_30s* /scratch/jms53460/11_2024_Sl/Demultiplexed
mv S1-8_A267-277*_31s* /scratch/jms53460/11_2024_Sl/Demultiplexed
mv S1-8_A267-277*_32s* /scratch/jms53460/11_2024_Sl/Demultiplexed
cp S1-8_A267-277*un*.fastq.gz /scratch/jms53460/11_2024_Sl/Demultiplexed

cd ..


#!/bin/bash
#SBATCH --job-name=At_hisat2                                         # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=6                                                 # Number of cores per task
#SBATCH --mem=100gb                                                       # Total memory for job
#SBATCH --time=12:00:00                                                   # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/11_2024_At/At_hs2.out                  # Location of standard output file
#SBATCH --error=/scratch/jms53460/11_2024_At/At_hs2.err                   # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=END,FAIL                                              # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/11_2024_At/

module load fastp/0.23.2-GCC-11.3.0
mkdir hisat2_out
for file in Demultiplexed/*s.fastq.gz; do
	file2="${file:14:-9}"

if [ ! -f "hisat2_out/""$file2"".bam" ]; then

	fastp -w 6 -i "$file" -o "hisat2_out/""$file2"".fastq.gz" -y -x -3 -a AAAAAAAAAAAA

fi
done

ml HISAT2/3n-20201216-gompi-2022a
ml SAMtools/1.16.1-GCC-11.3.0
for file in hisat2_out/*s.fastq.gz
do
	file2="${file:11:-9}"

if [ ! -f "hisat2_out/""$file2"".bam" ]; then

	hisat2 -p 6 --dta -x Ler_0_N-masked/merged_hisat2_index -U "hisat2_out/""$file2"".fastq.gz" | samtools view -bS -> "hisat2_out/""$file2""_unsorted.bam"
	samtools sort -@ 6 "hisat2_out/""$file2""_unsorted.bam" -o "hisat2_out/""$file2""_s.bam"
    samtools index -@ 6 "hisat2_out/""$file2""_s.bam"
	
fi
done


###Running SNPsplit

#!/bin/bash
#SBATCH --job-name=At_SNPsplit                                                      # Job name
#SBATCH --partition=batch                                                           # Partition (queue) name
#SBATCH --ntasks=1                                                                  # Single task job
#SBATCH --cpus-per-task=6                                                           # Number of cores per task
#SBATCH --mem=50gb                                                                  # Total memory for job
#SBATCH --time=6:00:00                                                             # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/11_2024_At/At_SNPsplit.out                       # Location of standard output file
#SBATCH --error=/scratch/jms53460/11_2024_At/At_SNPsplit.err                        # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                                # Where to send mail
#SBATCH --mail-type=END,FAIL                                                        # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/11_2024_At/
mkdir At_SNPsplit
ml SAMtools/1.16.1-GCC-11.3.0
ml SNPsplit/0.6.0-GCC-11.3.0-Perl-5.34.1
for file in "hisat2_out/"*_s.bam
do
    file2="${file:11:-6}"

    SNPsplit --conflicting -o At_SNPsplit --snp_file Ler_SNPs.tab "$file"
    samtools sort -@ 6 At_SNPsplit/"$file2"_s.allele_flagged.bam -o At_SNPsplit/"$file2"_SNPsplit.bam
    
done

for file in "At_SNPsplit/"*_s.genome1.bam
do
    file2="${file:12:-14}"
    samtools sort -@ 6 "$file" -o At_SNPsplit/"$file2"_SNPsplit_g1.bam
done

for file in "At_SNPsplit/"*_s.genome2.bam
do
    file2="${file:12:-14}"
    samtools sort -@ 6 "$file" -o At_SNPsplit/"$file2"_SNPsplit_g2.bam
done


#!/bin/bash
#SBATCH --job-name=At_features_UMIs                                           # Job name
#SBATCH --partition=batch                                                     # Partition (queue) name
#SBATCH --ntasks=1                                                            # Single task job
#SBATCH --cpus-per-task=6                                                     # Number of cores per task
#SBATCH --mem=50gb                                                            # Total memory for job
#SBATCH --time=12:00:00                                                       # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/11_2024_At/At_features_UMIs.out           # Location of standard output file
#SBATCH --error=/scratch/jms53460/11_2024_At/At_features_UMIs.err            # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                          # Where to send mail
#SBATCH --mail-type=END,FAIL                                                  # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/11_2024_At/
mkdir featurecounts
mkdir bams
mkdir UMIcounts
mkdir UMIcounts_g1
mkdir UMIcounts_g2
ml purge_dups/1.2.5-foss-2021b
ml Miniconda3/23.5.2-0
source activate /home/jms53460/subread-env

featureCounts -T 6 -s 1 -a TAIR10.1_Col_5.gff -t 'gene' -g 'ID' -o featurecounts/read_counts.tab --readExtension5 500 -R BAM At_SNPsplit/*_SNPsplit.bam
featureCounts -T 6 -s 1 -a TAIR10.1_Col_5.gff -t 'gene' -g 'ID' -o featurecounts/read_counts_g1.tab --readExtension5 500 -R BAM At_SNPsplit/*_SNPsplit_g1.bam
featureCounts -T 6 -s 1 -a TAIR10.1_Col_5.gff -t 'gene' -g 'ID' -o featurecounts/read_counts_g2.tab --readExtension5 500 -R BAM At_SNPsplit/*_SNPsplit_g2.bam

conda deactivate

for file in "featurecounts/"*SNPsplit.bam*
do
    file2="${file:14:-22}"
    if [ ! -f "UMIcounts/${file2}.tsv" ]; then

        module load SAMtools/1.16.1-GCC-11.3.0
        samtools sort -@ 6 "$file" -o "bams/$file2"
        samtools index "bams/$file2"

        module load UMI-tools/1.1.2-foss-2022a-Python-3.10.4
        umi_tools count --per-gene --gene-tag=XT --assigned-status-tag=XS -I "bams/$file2" -S "UMIcounts/${file2}.tsv"
    fi
done

for file in "featurecounts/"*g1.bam*
do
    file2="${file:14:-22}"
    if [ ! -f "UMIcounts_g1/${file2}.tsv" ]; then

        module load SAMtools/1.16.1-GCC-11.3.0
        samtools sort -@ 6 "$file" -o "bams/$file2"
        samtools index "bams/$file2"

        module load UMI-tools/1.1.2-foss-2022a-Python-3.10.4
        umi_tools count --per-gene --gene-tag=XT --assigned-status-tag=XS -I "bams/$file2" -S "UMIcounts_g1/${file2}.tsv"
    fi
done

for file in "featurecounts/"*g2.bam*
do
    file2="${file:14:-22}"
    if [ ! -f "UMIcounts_g2/${file2}.tsv" ]; then

        module load SAMtools/1.16.1-GCC-11.3.0
        samtools sort -@ 6 "$file" -o "bams/$file2"
        samtools index "bams/$file2"

        module load UMI-tools/1.1.2-foss-2022a-Python-3.10.4
        umi_tools count --per-gene --gene-tag=XT --assigned-status-tag=XS -I "bams/$file2" -S "UMIcounts_g2/${file2}.tsv"
    fi
done


ml R/4.3.1-foss-2022a
R
annots = strsplit(read.table('TAIR10.1_Col_5.gff', sep = '\t', quote = "")[,9], ';')
annots = annots[grep('ID=gene-', annots)]
names(annots) = unlist(lapply(annots, function(xx) { xx[1] }))
names(annots) = sub('ID=', '', names(annots))
annots = annots[!duplicated(names(annots))]
annots = sub(';', '', sub(' ', '', unlist(lapply(annots, function(xx) { sub('.+ ', '', if (length(xx) == 3) { xx[3] } else { xx[1] }) }))))

files = dir('UMIcounts')
A = matrix(NA, nrow = length(annots), ncol = length(files))
rownames(A) = annots
colnames(A) = files
for (f in files) {
	xx = read.table(paste('UMIcounts/', f, sep = ''), sep = '\t', header=T, row.names=1)
	A[,f] = xx[match(names(annots),rownames(xx)),1]
}
colnames(A) = sub('L003_', '', sub('_SNPsplit.tsv', '', files))
A[is.na(A)] = 0
#A = A[rowSums(A) > 0,]

B = A[order(rownames(A)),]
B2 = B
B = B[!duplicated(rownames(B)),]
for (g in unique(rownames(B)[duplicated(rownames(B))])) {
	B[g,] = colSums(B2[rownames(B2) %in% g,])
}
D = B

files = dir('UMIcounts_g1')
A = matrix(NA, nrow = length(annots), ncol = length(files))
rownames(A) = annots
colnames(A) = files
for (f in files) {
	xx = read.table(paste('UMIcounts_g1/', f, sep = ''), sep = '\t', header=T, row.names=1)
	A[,f] = xx[match(names(annots),rownames(xx)),1]
}
colnames(A) = sub('L003_', '', sub('_SNPsplit_g1.tsv', '', files))
A[is.na(A)] = 0
#A = A[rowSums(A) > 0,]

B = A[order(rownames(A)),]
B2 = B
B = B[!duplicated(rownames(B)),]
for (g in unique(rownames(B)[duplicated(rownames(B))])) {
	B[g,] = colSums(B2[rownames(B2) %in% g,])
}
g1 = B

files = dir('UMIcounts_g2')
A = matrix(NA, nrow = length(annots), ncol = length(files))
rownames(A) = annots
colnames(A) = files
for (f in files) {
	xx = read.table(paste('UMIcounts_g2/', f, sep = ''), sep = '\t', header=T, row.names=1)
	A[,f] = xx[match(names(annots),rownames(xx)),1]
}
colnames(A) = sub('L003_', '', sub('_SNPsplit_g2.tsv', '', files))
A[is.na(A)] = 0
#A = A[rowSums(A) > 0,]

B = A[order(rownames(A)),]
B2 = B
B = B[!duplicated(rownames(B)),]
for (g in unique(rownames(B)[duplicated(rownames(B))])) {
	B[g,] = colSums(B2[rownames(B2) %in% g,])
}
g2 = B

genes = read.table('TAIR10.1_Col_5.gff', sep = '\t', quote = "")[,c(1,5)]
annots2 = strsplit(read.table('TAIR10.1_Col_5.gff', sep = '\t', quote = "")[,9], ';')
names(annots2) = unlist(lapply(annots2, function(xx) { xx[1] }))
annots2 = sub(';', '', sub(' ', '', unlist(lapply(annots2, function(xx) { sub('.+ ', '', if (length(xx) == 3) { xx[3] } else { xx[1] }) }))))
genes[,3] = annots2
genes2 = genes[grepl('ID=gene-', genes[,3]),]
genes = genes2[order(genes2[,2]),] #order by position
genes = genes[order(genes[,1]),] #order by chr
genes = genes[!duplicated(genes[,3]),]
colnames(genes) = c('Chr', 'Position', 'Gene')
rownames(genes) = genes[,3]

D = D[rownames(genes),]
g1 = g1[rownames(genes),]
g2 = g2[rownames(genes),]

save(D,g1,g2,genes, file = "11_2024_At.RData")
q()



###Copying this to my local computer
scp sapelo2:/scratch/jms53460/11_2024_At/11_2024_At.RData 'C:/Users/julia/OneDrive/Desktop/Grad School/Nelms lab/Bioinformatics/R'

###In local R terminal
setwd('C:/Users/julia/OneDrive/Desktop/Grad School/Nelms lab/Bioinformatics/R')
load('C:/Users/julia/OneDrive/Desktop/Grad School/Nelms lab/Bioinformatics/R/Julian R instance.rda') #This has data from the first 48 Arabidopsis samples I sequenced alongside functions and such that were used for analysis
load('11_2024_At.RData')


plotCell2 = function (cell) 
{
    annotate_figure(ggarrange(plotChr(cell, chr = 1), plotChr(cell, 
        chr = 2), plotChr(cell, chr = 3), plotChr(cell, chr = 4), 
        plotChr(cell, chr = 5), plotScaleBar, ncol = 1, nrow = 6, 
        align = "v", heights = c(rep(1, 5), 0.4)), left = text_grob("          % Transcripts from Col-0 allele", 
        rot = 90, size = 10), top = cell)
}


BIN2 = function (xx, bin = 10^6) 
{
    bin = as.numeric(genes[, 1]) * 10^6 + round(genes[, 2]/bin)
    out = by(xx, bin, colSums)
    out2 = t(matrix(unlist(out), nrow = ncol(g1)))
    colnames(out2) = colnames(g1)
    rownames(out2) = names(out)
    return(out2)
}

library(ggplot2)
library(ggpubr)
g1_bin = BIN2(g1)
g2_bin = BIN2(g2)
g1_frac = g1_bin/(g1_bin + g2_bin)
AlleleFrac = g1_frac
AlleleFrac[(g1_bin+g2_bin) < 10] = NA #remove bins with <10 genoinformative transcripts
#AlleleFrac2 = AlleleFrac[,which(colSums(is.na(AlleleFrac)) <= 50)]
AlleleFrac2 = AlleleFrac[,which(colSums(D) >= 5000)] #68/160 pass this threshold. 25/160 pass >= 10000
###478/672 passed >=5000, 318/672 passed >=10000 with the last data set

summary(colSums(D))
#   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#    408    1838    4070    7360    7440   74137

library('ComplexHeatmap')

FracMono = 100*colMeans(abs(AlleleFrac2 - .5) >= .3, na.rm=T)

Heatmap(AlleleFrac2, cluster_rows=F)                   



A155-168_S94_L003_R1_001.fastq.gz                                                        100% 5686MB  11.5MB/s   08:16    
A155-168_S94_L003_R2_001.fastq.gz                                                        100% 5512MB  11.4MB/s   08:02    
A182-193_S33_L005_R1_001.fastq.gz                                                        100% 4339MB   8.9MB/s   08:07    
A182-193_S33_L005_R2_001.fastq.gz                                                        100% 4324MB   8.5MB/s   08:31    
A194-205_S34_L005_R1_001.fastq.gz                                                        100% 3957MB   9.8MB/s   06:44    
A194-205_S34_L005_R2_001.fastq.gz                                                        100% 4002MB  12.2MB/s   05:28    
A206-217_S35_L005_R1_001.fastq.gz                                                        100% 2183MB  12.5MB/s   02:54    
A206-217_S35_L005_R2_001.fastq.gz                                                        100% 2174MB  12.8MB/s   02:49    
A218-229_S36_L005_R1_001.fastq.gz                                                        100% 3374MB  12.9MB/s   04:21    
A218-229_S36_L005_R2_001.fastq.gz                                                        100% 3267MB  12.0MB/s   04:32    
A230-241_S37_L005_R1_001.fastq.gz                                                        100% 4296MB  13.3MB/s   05:23    
A230-241_S37_L005_R2_001.fastq.gz                                                        100% 4370MB  13.2MB/s   05:31    
A242-253_S38_L005_R1_001.fastq.gz                                                        100% 3699MB  12.9MB/s   04:47    
A242-253_S38_L005_R2_001.fastq.gz                                                        100% 3821MB  13.2MB/s   04:50    
PS C:\Users\julia\OneDrive\Documents\GitHub\julian> scp sapelo2:/scratch/jms53460/7_2024_Ns/Raw_Data/*.fastq.gz 'D:\7_2024_Ns_Data'
T104-115_S95_L003_R1_001.fastq.gz                                                        100% 5221MB  11.3MB/s   07:42    
T104-115_S95_L003_R2_001.fastq.gz                                                        100% 5351MB  11.2MB/s   07:56    
PS C:\Users\julia\OneDrive\Documents\GitHub\julian> scp sapelo2:/scratch/jms53460/11_2024_At/Raw_Data/*.fastq.gz 'D:\11_2024_At_Sl_Data'
A254-266_S2_L002_R1_001.fastq.gz                                                         100% 1420MB  10.6MB/s   02:14    
A254-266_S2_L002_R2_001.fastq.gz                                                         100% 1370MB  11.1MB/s   02:03    
S1-8_A267-277_S3_L002_R1_001.fastq.gz                                                    100% 1455MB  11.0MB/s   02:11    
S1-8_A267-277_S3_L002_R2_001.fastq.gz                                                    100% 1384MB  12.0MB/s   01:55

~72 GB of data currently. For future runs, each set of 96 (one plate) will probably take 3-5 GB of data.
In addition to the current data, I'll be getting sequencing data from 6 plates Arabidopsis, 10 tomato, 10 tobacco, 10 rice
Assuming 5 GB of data per plate, 36 plates should take 180 GB. 
In total, I probably need about 252 GB of data storage.





