###Download Maize scRNA-seq data

#!/bin/bash
#SBATCH --job-name=download_maize_data                  # Job name
#SBATCH --partition=batch                               # Partition (queue) name
#SBATCH --ntasks=1                                      # Single task job
#SBATCH --cpus-per-task=1                               # Number of cores per task
#SBATCH --mem=50gb                                      # Total memory for job
#SBATCH --time=72:00:00                                 # Time limit hrs:min:sec
#SBATCH --output=/home/jms53460/Zm_download.out         # Location of standard output file
#SBATCH --error=/home/jms53460/Zm_download.err          # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                    # Where to send mail
#SBATCH --mail-type=END,FAIL                            # Mail events (BEGIN, END, FAIL, ALL)

cd /home/jms53460/Maize_SGT_2022
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_CKDL190144505-1B-10_HNLFWDSXX_L3_R1.fastq.gz > RPI10_1B-10_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_CKDL190144505-1B-10_HNLFWDSXX_L3_R2.fastq.gz > RPI10_1B-10_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_TAGCTT_S6_L002_R1.fastq.gz > RPI10_S6_L002_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_TAGCTT_S6_L002_R2.fastq.gz > RPI10_S6_L002_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI11_GGCTAC_S5_L002_R1.fastq.gz > RPI11_S5_L002_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI11_GGCTAC_S5_L002_R2.fastq.gz > RPI11_S5_L002_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL190144505-1B-1_HNLFWDSXX_L3_R1.fastq.gz > RPI1_1B-1_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL190144505-1B-1_HNLFWDSXX_L3_R2.fastq.gz > RPI1_1B-1_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL200149798-1a_H7MM2BBXX_L2_R1.fastq.gz > RPI1_1a_L2_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL200149798-1a_H7MM2BBXX_L2_R2.fastq.gz > RPI1_1a_L2_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL190144505-1B-2_HNLFWDSXX_L3_R1.fastq.gz > RPI2_1B-2_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL190144505-1B-2_HNLFWDSXX_L3_R2.fastq.gz > RPI2_1B-2_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL200149799-1a_H7MM2BBXX_L3_R1.fastq.gz > RPI2_1a_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL200149799-1a_H7MM2BBXX_L3_R2.fastq.gz > RPI2_1a_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI3_CKDL190144505-1B-3_HNLFWDSXX_L3_R1.fastq.gz > RPI3_1B-3_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI3_CKDL190144505-1B-3_HNLFWDSXX_L3_R2.fastq.gz > RPI3_1B-3_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI4_CKDL190144505-1B-4_HNLFWDSXX_L3_R1.fastq.gz > RPI4_1B-4_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI4_CKDL190144505-1B-4_HNLFWDSXX_L3_R2.fastq.gz > RPI4_1B-4_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI5_CKDL190144505-1B-5_HNLFWDSXX_L3_R1.fastq.gz > RPI5_1B-5_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI5_CKDL190144505-1B-5_HNLFWDSXX_L3_R2.fastq.gz > RPI5_1B-5_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI6_CKDL190144505-1B-6_HNLFWDSXX_L3_R1.fastq.gz > RPI6_1B-6_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI6_CKDL190144505-1B-6_HNLFWDSXX_L3_R2.fastq.gz > RPI6_1B-6_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI7_CKDL190144505-1B-7_HNLFWDSXX_L3_R1.fastq.gz > RPI7_1B-7_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI7_CKDL190144505-1B-7_HNLFWDSXX_L3_R2.fastq.gz > RPI7_1B-7_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI8_CKDL190144505-1B-8_HNLFWDSXX_L3_R1.fastq.gz > RPI8_1B-8_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI8_CKDL190144505-1B-8_HNLFWDSXX_L3_R2.fastq.gz > RPI8_1B-8_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI9_CKDL190144505-1B-9_HNLFWDSXX_L3_R1.fastq.gz > RPI9_1B-9_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI9_CKDL190144505-1B-9_HNLFWDSXX_L3_R2.fastq.gz > RPI9_1B-9_L3_R2.fastq.gz


#!/bin/bash
#SBATCH --job-name=download_maize_data                  # Job name
#SBATCH --partition=batch                               # Partition (queue) name
#SBATCH --ntasks=1                                      # Single task job
#SBATCH --cpus-per-task=1                               # Number of cores per task
#SBATCH --mem=50gb                                      # Total memory for job
#SBATCH --time=72:00:00                                 # Time limit hrs:min:sec
#SBATCH --output=/home/jms53460/Zm_download2.out         # Location of standard output file
#SBATCH --error=/home/jms53460/Zm_download2.err          # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                    # Where to send mail
#SBATCH --mail-type=END,FAIL                            # Mail events (BEGIN, END, FAIL, ALL)

###I stopped the job so I could run other jobs for tobacco, and I deleted the last file being downloaded since was incomplete

mv /home/jms53460/Maize_SGT_2022/*.gz /scratch/jms53460/Maize_SGT_2022
cd /scratch/jms53460/Maize_SGT_2022
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_CKDL190144505-1B-10_HNLFWDSXX_L3_R1.fastq.gz > RPI10_1B-10_L3_R1.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_CKDL190144505-1B-10_HNLFWDSXX_L3_R2.fastq.gz > RPI10_1B-10_L3_R2.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_TAGCTT_S6_L002_R1.fastq.gz > RPI10_S6_L002_R1.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_TAGCTT_S6_L002_R2.fastq.gz > RPI10_S6_L002_R2.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI11_GGCTAC_S5_L002_R1.fastq.gz > RPI11_S5_L002_R1.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI11_GGCTAC_S5_L002_R2.fastq.gz > RPI11_S5_L002_R2.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL190144505-1B-1_HNLFWDSXX_L3_R1.fastq.gz > RPI1_1B-1_L3_R1.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL190144505-1B-1_HNLFWDSXX_L3_R2.fastq.gz > RPI1_1B-1_L3_R2.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL200149798-1a_H7MM2BBXX_L2_R1.fastq.gz > RPI1_1a_L2_R1.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL200149798-1a_H7MM2BBXX_L2_R2.fastq.gz > RPI1_1a_L2_R2.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL190144505-1B-2_HNLFWDSXX_L3_R1.fastq.gz > RPI2_1B-2_L3_R1.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL190144505-1B-2_HNLFWDSXX_L3_R2.fastq.gz > RPI2_1B-2_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL200149799-1a_H7MM2BBXX_L3_R1.fastq.gz > RPI2_1a_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL200149799-1a_H7MM2BBXX_L3_R2.fastq.gz > RPI2_1a_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI3_CKDL190144505-1B-3_HNLFWDSXX_L3_R1.fastq.gz > RPI3_1B-3_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI3_CKDL190144505-1B-3_HNLFWDSXX_L3_R2.fastq.gz > RPI3_1B-3_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI4_CKDL190144505-1B-4_HNLFWDSXX_L3_R1.fastq.gz > RPI4_1B-4_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI4_CKDL190144505-1B-4_HNLFWDSXX_L3_R2.fastq.gz > RPI4_1B-4_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI5_CKDL190144505-1B-5_HNLFWDSXX_L3_R1.fastq.gz > RPI5_1B-5_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI5_CKDL190144505-1B-5_HNLFWDSXX_L3_R2.fastq.gz > RPI5_1B-5_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI6_CKDL190144505-1B-6_HNLFWDSXX_L3_R1.fastq.gz > RPI6_1B-6_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI6_CKDL190144505-1B-6_HNLFWDSXX_L3_R2.fastq.gz > RPI6_1B-6_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI7_CKDL190144505-1B-7_HNLFWDSXX_L3_R1.fastq.gz > RPI7_1B-7_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI7_CKDL190144505-1B-7_HNLFWDSXX_L3_R2.fastq.gz > RPI7_1B-7_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI8_CKDL190144505-1B-8_HNLFWDSXX_L3_R1.fastq.gz > RPI8_1B-8_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI8_CKDL190144505-1B-8_HNLFWDSXX_L3_R2.fastq.gz > RPI8_1B-8_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI9_CKDL190144505-1B-9_HNLFWDSXX_L3_R1.fastq.gz > RPI9_1B-9_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI9_CKDL190144505-1B-9_HNLFWDSXX_L3_R2.fastq.gz > RPI9_1B-9_L3_R2.fastq.gz


###Download B73 v5 genome and A188 genome
cd /scratch/jms53460/Maize_SGT_2022
curl -s https://download.maizegdb.org/Zm-B73-REFERENCE-NAM-5.0/Zm-B73-REFERENCE-NAM-5.0.fa.gz > B73_v5_genome.fa.gz
curl -s https://download.maizegdb.org/Zm-A188-REFERENCE-KSU-1.0/Zm-A188-REFERENCE-KSU-1.0.fa.gz > A188_genome.fa.gz


###Trying to map the A188 genome to the B73 genome so I have a bam file
mkdir hisat2_out
gzip -d B73_v5_genome.fa.gz
gzip -d A188_genome.fa.gz

#!/bin/bash
#SBATCH --job-name=Zm_genome_hisat2                                      # Job name
#SBATCH --partition=batch                                                # Partition (queue) name
#SBATCH --ntasks=1                                                       # Single task job
#SBATCH --cpus-per-task=6                                                # Number of cores per task
#SBATCH --mem=50gb                                                       # Total memory for job
#SBATCH --time=12:00:00                                                  # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_genome_hisat2.out   # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_genome_hisat2.err    # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                     # Where to send mail
#SBATCH --mail-type=END,FAIL                                             # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
ml HISAT2/3n-20201216-gompi-2022a
hisat2-build B73_v5_genome.fa B73_v5_hisat2_index

module load SAMtools/1.16.1-GCC-11.3.0
hisat2 -p 6 --dta -x B73_v5_hisat2_index -U A188_genome.fa | samtools view -bS -> hisat2_out/A188_B73_unsorted.bam
samtools sort -@ 6 hisat2_out/A188_B73_unsorted.bam -o A188_B73_s.bam


###The index was made fine, but hisat2 did not work (probably because this is not what it is meant to do).
rm hisat2_out/A188_B73_unsorted.bam


###Simulating sequence reads from A188 genome so I can align them to the B73 genome and proceed with the next steps

#!/bin/bash
#SBATCH --job-name=Zm_genome_hisat2_2                                    # Job name
#SBATCH --partition=batch                                                # Partition (queue) name
#SBATCH --ntasks=1                                                       # Single task job
#SBATCH --cpus-per-task=6                                                # Number of cores per task
#SBATCH --mem=50gb                                                       # Total memory for job
#SBATCH --time=12:00:00                                                  # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_genome_hisat2_2.out # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_genome_hisat2_2.err  # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                     # Where to send mail
#SBATCH --mail-type=END,FAIL                                             # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022

ml wgsim/20111017-GCC-11.3.0
wgsim A188_genome.fa A188_sim.read1.fq A188_sim.read2.fq 
gzip A188_sim.read1.fq
gzip A188_sim.read2.fq

ml HISAT2/3n-20201216-gompi-2022a
ml SAMtools/1.16.1-GCC-11.3.0
hisat2 -p 6 --dta -x B73_v5_hisat2_index -1 A188_sim.read1.fq.gz -2 A188_sim.read2.fq.gz | samtools view -bS -> hisat2_out/A188_B73_unsorted.bam
samtools sort -@ 6 hisat2_out/A188_B73_unsorted.bam -o A188_B73_s.bam


###Make vcf file

#!/bin/bash
#SBATCH --job-name=Zm_vcf                               # Job name
#SBATCH --partition=batch                               # Partition (queue) name
#SBATCH --ntasks=1                                      # Single task job
#SBATCH --cpus-per-task=6                               # Number of cores per task
#SBATCH --mem=50gb                                      # Total memory for job
#SBATCH --time=12:00:00                                 # Time limit hrs:min:sec
#SBATCH --output=/home/jms53460/Zm_vcf.out              # Location of standard output file
#SBATCH --error=/home/jms53460/Zm_vcf.err               # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                    # Where to send mail
#SBATCH --mail-type=END,FAIL                            # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
module load SAMtools/1.16.1-GCC-11.3.0
samtools index -@ 6 A188_B73_s.bam

module load BCFtools/1.15.1-GCC-11.3.0
bcftools mpileup -Ou -d 1000000 --threads 6 --min-MQ 60 -f B73_v5_genome.fa A188_B73_s.bam | bcftools call -Ou -m -v --threads 6 | bcftools filter -Oz -e 'QUAL<40 || DP<10' > Zm_A188_B73_vcf.gz
bcftools index Zm_A188_B73_vcf.gz


###I only got two SNPs in the vcf file, so this is clearly too little depth. I will try again specifying longer reads and enabling bcftools mpileup to use up to 1,000,000 reads from one file (default is 250)

rm *gz* #delete the old simulated files, vcf, and vcf index
rm A188_B73_s* #delete the old bam and index
rm hisat2_out/A188_B73_unsorted.bam 

#!/bin/bash
#SBATCH --job-name=Zm_genome_hisat2_vcf                                    # Job name
#SBATCH --partition=batch                                                  # Partition (queue) name
#SBATCH --ntasks=1                                                         # Single task job
#SBATCH --cpus-per-task=6                                                  # Number of cores per task
#SBATCH --mem=50gb                                                         # Total memory for job
#SBATCH --time=12:00:00                                                    # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_genome_hisat2_vcf.out # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_genome_hisat2_vcf.err  # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                       # Where to send mail
#SBATCH --mail-type=END,FAIL                                               # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022

ml wgsim/20111017-GCC-11.3.0
wgsim -1 10000 -2 10000 -d 20000 A188_genome.fa A188_sim.read1.fq A188_sim.read2.fq 
gzip A188_sim.read1.fq
gzip A188_sim.read2.fq

ml HISAT2/3n-20201216-gompi-2022a
ml SAMtools/1.16.1-GCC-11.3.0
hisat2 -p 6 --dta -x B73_v5_hisat2_index -1 A188_sim.read1.fq.gz -2 A188_sim.read2.fq.gz | samtools view -bS -> hisat2_out/A188_B73_unsorted.bam
samtools sort -@ 6 hisat2_out/A188_B73_unsorted.bam -o A188_B73_s.bam
samtools index -@ 6 A188_B73_s.bam

module load BCFtools/1.15.1-GCC-11.3.0
bcftools mpileup -Ou -d 1000000 --threads 6 --min-MQ 60 -f B73_v5_genome.fa A188_B73_s.bam | bcftools call -Ou -m -v --threads 6 | bcftools filter -Oz -e 'QUAL<40 || DP<10' > Zm_A188_B73_vcf.gz
bcftools index Zm_A188_B73_vcf.gz


#I stopped this and deleted the files made because I found this program SyRI that seems like a better option


###Mapping the genomes to eachother with minimap2 then using SyRI to output a vcf file

#!/bin/bash
#SBATCH --job-name=Zm_minimap_syri                                         # Job name
#SBATCH --partition=batch                                                  # Partition (queue) name
#SBATCH --ntasks=1                                                         # Single task job
#SBATCH --cpus-per-task=6                                                  # Number of cores per task
#SBATCH --mem=400gb                                                        # Total memory for job
#SBATCH --time=24:00:00                                                    # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_minimap_syri.out      # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_minimap_syri.err       # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                       # Where to send mail
#SBATCH --mail-type=END,FAIL                                               # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022

ml minimap2/2.26-GCCcore-12.2.0
minimap2 -t 6 -ax asm5 --eqx B73_v5_genome.fa A188_genome.fa > A188_aligned_to_B73.sam

ml SyRI/1.6.3
syri -c A188_aligned_to_B73.sam -r B73_v5_genome.fa -q A188_genome.fa -k -F S








###Demultiplex the raw data

###Install fastq-multx
ml Miniconda3/23.5.2-0
conda create -p /scratch/jms53460/Fastq-Multx -c bioconda fastq-multx #when prompted, say 'y'
source activate /scratch/jms53460/Fastq-Multx/
fastq-multx
conda deactivate

cd /scratch/jms53460/Maize_SGT_2022
mkdir Raw_Data
mkdir Mapped_Data
mkdir Mapped_Data/demultiplexed
mv *fastq.gz Raw_Data
cp /home/jms53460/CELSeq_barcodes.txt /scratch/jms53460/Maize_SGT_2022/


#!/bin/bash
#SBATCH --job-name=Zm_demultiplex                       # Job name
#SBATCH --partition=batch                               # Partition (queue) name
#SBATCH --ntasks=1                                      # Single task job
#SBATCH --cpus-per-task=6                               # Number of cores per task
#SBATCH --mem=200gb                                     # Total memory for job
#SBATCH --time=48:00:00                                 # Time limit hrs:min:sec
#SBATCH --output=/home/jms53460/Zm_dm.out               # Location of standard output file
#SBATCH --error=/home/jms53460/Zm_dm.err                # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                    # Where to send mail
#SBATCH --mail-type=END,FAIL                            # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
ml Miniconda3/23.5.2-0
source activate /scratch/jms53460/Fastq-Multx/

for file in Raw_Data/*_R1*.gz; do
    file2="${file:9:-12}"

    if [ ! -f "Mapped_Data/demultiplexed/""$file2""_dT-1s.fastq.gz" ]; then
        module load fastp/0.23.2-GCC-11.3.0
	    fastp -w 6 -i "$file" -I "Raw_Data/""$file2""_R2.fastq.gz" -o "Mapped_Data/demultiplexed/umi_""$file2""_R1.fastq.gz" -O "Mapped_Data/demultiplexed/umi_""$file2""_R2.fastq.gz" -A -Q -L --umi --umi_loc read1 --umi_len 10 --umi_prefix UMI

	    fastq-multx -b -B "CELSeq_barcodes.txt" -m 0 "Mapped_Data/demultiplexed/umi_""$file2""_R1.fastq.gz" "Mapped_Data/demultiplexed/umi_""$file2""_R2.fastq.gz" -o "Mapped_Data/demultiplexed/""$file2""_%_R1.fastq.gz" "Mapped_Data/demultiplexed/""$file2""_%.fastq.gz"  # Split read 2 file by CELseq barcodes. Require perfect match to barcode in expected location

	    find "Mapped_Data/demultiplexed/" -name "umi_*" -delete
	    find "Mapped_Data/demultiplexed/" -name "*_R1*" -delete
    fi
done
conda deactivate

