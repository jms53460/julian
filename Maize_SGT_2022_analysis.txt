###Download Maize scRNA-seq data

#!/bin/bash
#SBATCH --job-name=download_maize_data                  # Job name
#SBATCH --partition=batch                               # Partition (queue) name
#SBATCH --ntasks=1                                      # Single task job
#SBATCH --cpus-per-task=1                               # Number of cores per task
#SBATCH --mem=50gb                                      # Total memory for job
#SBATCH --time=72:00:00                                 # Time limit hrs:min:sec
#SBATCH --output=/home/jms53460/Zm_download.out         # Location of standard output file
#SBATCH --error=/home/jms53460/Zm_download.err          # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                    # Where to send mail
#SBATCH --mail-type=END,FAIL                            # Mail events (BEGIN, END, FAIL, ALL)

cd /home/jms53460/Maize_SGT_2022
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_CKDL190144505-1B-10_HNLFWDSXX_L3_R1.fastq.gz > RPI10_1B-10_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_CKDL190144505-1B-10_HNLFWDSXX_L3_R2.fastq.gz > RPI10_1B-10_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_TAGCTT_S6_L002_R1.fastq.gz > RPI10_S6_L002_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_TAGCTT_S6_L002_R2.fastq.gz > RPI10_S6_L002_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI11_GGCTAC_S5_L002_R1.fastq.gz > RPI11_S5_L002_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI11_GGCTAC_S5_L002_R2.fastq.gz > RPI11_S5_L002_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL190144505-1B-1_HNLFWDSXX_L3_R1.fastq.gz > RPI1_1B-1_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL190144505-1B-1_HNLFWDSXX_L3_R2.fastq.gz > RPI1_1B-1_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL200149798-1a_H7MM2BBXX_L2_R1.fastq.gz > RPI1_1a_L2_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL200149798-1a_H7MM2BBXX_L2_R2.fastq.gz > RPI1_1a_L2_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL190144505-1B-2_HNLFWDSXX_L3_R1.fastq.gz > RPI2_1B-2_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL190144505-1B-2_HNLFWDSXX_L3_R2.fastq.gz > RPI2_1B-2_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL200149799-1a_H7MM2BBXX_L3_R1.fastq.gz > RPI2_1a_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL200149799-1a_H7MM2BBXX_L3_R2.fastq.gz > RPI2_1a_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI3_CKDL190144505-1B-3_HNLFWDSXX_L3_R1.fastq.gz > RPI3_1B-3_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI3_CKDL190144505-1B-3_HNLFWDSXX_L3_R2.fastq.gz > RPI3_1B-3_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI4_CKDL190144505-1B-4_HNLFWDSXX_L3_R1.fastq.gz > RPI4_1B-4_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI4_CKDL190144505-1B-4_HNLFWDSXX_L3_R2.fastq.gz > RPI4_1B-4_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI5_CKDL190144505-1B-5_HNLFWDSXX_L3_R1.fastq.gz > RPI5_1B-5_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI5_CKDL190144505-1B-5_HNLFWDSXX_L3_R2.fastq.gz > RPI5_1B-5_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI6_CKDL190144505-1B-6_HNLFWDSXX_L3_R1.fastq.gz > RPI6_1B-6_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI6_CKDL190144505-1B-6_HNLFWDSXX_L3_R2.fastq.gz > RPI6_1B-6_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI7_CKDL190144505-1B-7_HNLFWDSXX_L3_R1.fastq.gz > RPI7_1B-7_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI7_CKDL190144505-1B-7_HNLFWDSXX_L3_R2.fastq.gz > RPI7_1B-7_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI8_CKDL190144505-1B-8_HNLFWDSXX_L3_R1.fastq.gz > RPI8_1B-8_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI8_CKDL190144505-1B-8_HNLFWDSXX_L3_R2.fastq.gz > RPI8_1B-8_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI9_CKDL190144505-1B-9_HNLFWDSXX_L3_R1.fastq.gz > RPI9_1B-9_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI9_CKDL190144505-1B-9_HNLFWDSXX_L3_R2.fastq.gz > RPI9_1B-9_L3_R2.fastq.gz


#!/bin/bash
#SBATCH --job-name=download_maize_data                  # Job name
#SBATCH --partition=batch                               # Partition (queue) name
#SBATCH --ntasks=1                                      # Single task job
#SBATCH --cpus-per-task=1                               # Number of cores per task
#SBATCH --mem=50gb                                      # Total memory for job
#SBATCH --time=72:00:00                                 # Time limit hrs:min:sec
#SBATCH --output=/home/jms53460/Zm_download2.out         # Location of standard output file
#SBATCH --error=/home/jms53460/Zm_download2.err          # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                    # Where to send mail
#SBATCH --mail-type=END,FAIL                            # Mail events (BEGIN, END, FAIL, ALL)

###I stopped the job so I could run other jobs for tobacco, and I deleted the last file being downloaded since was incomplete

mv /home/jms53460/Maize_SGT_2022/*.gz /scratch/jms53460/Maize_SGT_2022
cd /scratch/jms53460/Maize_SGT_2022
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_CKDL190144505-1B-10_HNLFWDSXX_L3_R1.fastq.gz > RPI10_1B-10_L3_R1.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_CKDL190144505-1B-10_HNLFWDSXX_L3_R2.fastq.gz > RPI10_1B-10_L3_R2.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_TAGCTT_S6_L002_R1.fastq.gz > RPI10_S6_L002_R1.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI10_TAGCTT_S6_L002_R2.fastq.gz > RPI10_S6_L002_R2.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI11_GGCTAC_S5_L002_R1.fastq.gz > RPI11_S5_L002_R1.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI11_GGCTAC_S5_L002_R2.fastq.gz > RPI11_S5_L002_R2.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL190144505-1B-1_HNLFWDSXX_L3_R1.fastq.gz > RPI1_1B-1_L3_R1.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL190144505-1B-1_HNLFWDSXX_L3_R2.fastq.gz > RPI1_1B-1_L3_R2.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL200149798-1a_H7MM2BBXX_L2_R1.fastq.gz > RPI1_1a_L2_R1.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL200149798-1a_H7MM2BBXX_L2_R2.fastq.gz > RPI1_1a_L2_R2.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL190144505-1B-2_HNLFWDSXX_L3_R1.fastq.gz > RPI2_1B-2_L3_R1.fastq.gz
#curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL190144505-1B-2_HNLFWDSXX_L3_R2.fastq.gz > RPI2_1B-2_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL200149799-1a_H7MM2BBXX_L3_R1.fastq.gz > RPI2_1a_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL200149799-1a_H7MM2BBXX_L3_R2.fastq.gz > RPI2_1a_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI3_CKDL190144505-1B-3_HNLFWDSXX_L3_R1.fastq.gz > RPI3_1B-3_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI3_CKDL190144505-1B-3_HNLFWDSXX_L3_R2.fastq.gz > RPI3_1B-3_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI4_CKDL190144505-1B-4_HNLFWDSXX_L3_R1.fastq.gz > RPI4_1B-4_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI4_CKDL190144505-1B-4_HNLFWDSXX_L3_R2.fastq.gz > RPI4_1B-4_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI5_CKDL190144505-1B-5_HNLFWDSXX_L3_R1.fastq.gz > RPI5_1B-5_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI5_CKDL190144505-1B-5_HNLFWDSXX_L3_R2.fastq.gz > RPI5_1B-5_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI6_CKDL190144505-1B-6_HNLFWDSXX_L3_R1.fastq.gz > RPI6_1B-6_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI6_CKDL190144505-1B-6_HNLFWDSXX_L3_R2.fastq.gz > RPI6_1B-6_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI7_CKDL190144505-1B-7_HNLFWDSXX_L3_R1.fastq.gz > RPI7_1B-7_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI7_CKDL190144505-1B-7_HNLFWDSXX_L3_R2.fastq.gz > RPI7_1B-7_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI8_CKDL190144505-1B-8_HNLFWDSXX_L3_R1.fastq.gz > RPI8_1B-8_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI8_CKDL190144505-1B-8_HNLFWDSXX_L3_R2.fastq.gz > RPI8_1B-8_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI9_CKDL190144505-1B-9_HNLFWDSXX_L3_R1.fastq.gz > RPI9_1B-9_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI9_CKDL190144505-1B-9_HNLFWDSXX_L3_R2.fastq.gz > RPI9_1B-9_L3_R2.fastq.gz


###Download B73 v5 genome and A188 genome
cd /scratch/jms53460/Maize_SGT_2022
curl -s https://download.maizegdb.org/Zm-B73-REFERENCE-NAM-5.0/Zm-B73-REFERENCE-NAM-5.0.fa.gz > B73_v5_genome.fa.gz
curl -s https://download.maizegdb.org/Zm-A188-REFERENCE-KSU-1.0/Zm-A188-REFERENCE-KSU-1.0.fa.gz > A188_genome.fa.gz


###Trying to map the A188 genome to the B73 genome so I have a bam file
mkdir hisat2_out
gzip -d B73_v5_genome.fa.gz
gzip -d A188_genome.fa.gz

#!/bin/bash
#SBATCH --job-name=Zm_genome_hisat2                                      # Job name
#SBATCH --partition=batch                                                # Partition (queue) name
#SBATCH --ntasks=1                                                       # Single task job
#SBATCH --cpus-per-task=6                                                # Number of cores per task
#SBATCH --mem=50gb                                                       # Total memory for job
#SBATCH --time=12:00:00                                                  # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_genome_hisat2.out   # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_genome_hisat2.err    # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                     # Where to send mail
#SBATCH --mail-type=END,FAIL                                             # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
ml HISAT2/3n-20201216-gompi-2022a
hisat2-build B73_v5_genome.fa B73_v5_hisat2_index

module load SAMtools/1.16.1-GCC-11.3.0
hisat2 -p 6 --dta -x B73_v5_hisat2_index -U A188_genome.fa | samtools view -bS -> hisat2_out/A188_B73_unsorted.bam
samtools sort -@ 6 hisat2_out/A188_B73_unsorted.bam -o A188_B73_s.bam


###The index was made fine, but hisat2 did not work (probably because this is not what it is meant to do).
rm hisat2_out/A188_B73_unsorted.bam


###Simulating sequence reads from A188 genome so I can align them to the B73 genome and proceed with the next steps

#!/bin/bash
#SBATCH --job-name=Zm_genome_hisat2_2                                    # Job name
#SBATCH --partition=batch                                                # Partition (queue) name
#SBATCH --ntasks=1                                                       # Single task job
#SBATCH --cpus-per-task=6                                                # Number of cores per task
#SBATCH --mem=50gb                                                       # Total memory for job
#SBATCH --time=12:00:00                                                  # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_genome_hisat2_2.out # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_genome_hisat2_2.err  # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                     # Where to send mail
#SBATCH --mail-type=END,FAIL                                             # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022

ml wgsim/20111017-GCC-11.3.0
wgsim A188_genome.fa A188_sim.read1.fq A188_sim.read2.fq 
gzip A188_sim.read1.fq
gzip A188_sim.read2.fq

ml HISAT2/3n-20201216-gompi-2022a
ml SAMtools/1.16.1-GCC-11.3.0
hisat2 -p 6 --dta -x B73_v5_hisat2_index -1 A188_sim.read1.fq.gz -2 A188_sim.read2.fq.gz | samtools view -bS -> hisat2_out/A188_B73_unsorted.bam
samtools sort -@ 6 hisat2_out/A188_B73_unsorted.bam -o A188_B73_s.bam


###Make vcf file

#!/bin/bash
#SBATCH --job-name=Zm_vcf                               # Job name
#SBATCH --partition=batch                               # Partition (queue) name
#SBATCH --ntasks=1                                      # Single task job
#SBATCH --cpus-per-task=6                               # Number of cores per task
#SBATCH --mem=50gb                                      # Total memory for job
#SBATCH --time=12:00:00                                 # Time limit hrs:min:sec
#SBATCH --output=/home/jms53460/Zm_vcf.out              # Location of standard output file
#SBATCH --error=/home/jms53460/Zm_vcf.err               # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                    # Where to send mail
#SBATCH --mail-type=END,FAIL                            # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
module load SAMtools/1.16.1-GCC-11.3.0
samtools index -@ 6 A188_B73_s.bam

module load BCFtools/1.15.1-GCC-11.3.0
bcftools mpileup -Ou -d 1000000 --threads 6 --min-MQ 60 -f B73_v5_genome.fa A188_B73_s.bam | bcftools call -Ou -m -v --threads 6 | bcftools filter -Oz -e 'QUAL<40 || DP<10' > Zm_A188_B73_vcf.gz
bcftools index Zm_A188_B73_vcf.gz


###I only got two SNPs in the vcf file, so this is clearly too little depth. I will try again specifying longer reads and enabling bcftools mpileup to use up to 1,000,000 reads from one file (default is 250)

rm *gz* #delete the old simulated files, vcf, and vcf index
rm A188_B73_s* #delete the old bam and index
rm hisat2_out/A188_B73_unsorted.bam 

#!/bin/bash
#SBATCH --job-name=Zm_genome_hisat2_vcf                                    # Job name
#SBATCH --partition=batch                                                  # Partition (queue) name
#SBATCH --ntasks=1                                                         # Single task job
#SBATCH --cpus-per-task=6                                                  # Number of cores per task
#SBATCH --mem=50gb                                                         # Total memory for job
#SBATCH --time=12:00:00                                                    # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_genome_hisat2_vcf.out # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_genome_hisat2_vcf.err  # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                       # Where to send mail
#SBATCH --mail-type=END,FAIL                                               # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022

ml wgsim/20111017-GCC-11.3.0
wgsim -1 10000 -2 10000 -d 20000 A188_genome.fa A188_sim.read1.fq A188_sim.read2.fq 
gzip A188_sim.read1.fq
gzip A188_sim.read2.fq

ml HISAT2/3n-20201216-gompi-2022a
ml SAMtools/1.16.1-GCC-11.3.0
hisat2 -p 6 --dta -x B73_v5_hisat2_index -1 A188_sim.read1.fq.gz -2 A188_sim.read2.fq.gz | samtools view -bS -> hisat2_out/A188_B73_unsorted.bam
samtools sort -@ 6 hisat2_out/A188_B73_unsorted.bam -o A188_B73_s.bam
samtools index -@ 6 A188_B73_s.bam

module load BCFtools/1.15.1-GCC-11.3.0
bcftools mpileup -Ou -d 1000000 --threads 6 --min-MQ 60 -f B73_v5_genome.fa A188_B73_s.bam | bcftools call -Ou -m -v --threads 6 | bcftools filter -Oz -e 'QUAL<40 || DP<10' > Zm_A188_B73_vcf.gz
bcftools index Zm_A188_B73_vcf.gz


#I stopped this and deleted the files made because I found this program SyRI that seems like a better option


###Mapping the genomes to eachother with minimap2 then using SyRI to output a vcf file

#!/bin/bash
#SBATCH --job-name=Zm_minimap_syri                                         # Job name
#SBATCH --partition=batch                                                  # Partition (queue) name
#SBATCH --ntasks=1                                                         # Single task job
#SBATCH --cpus-per-task=6                                                  # Number of cores per task
#SBATCH --mem=400gb                                                        # Total memory for job
#SBATCH --time=24:00:00                                                    # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_minimap_syri.out      # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_minimap_syri.err       # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                       # Where to send mail
#SBATCH --mail-type=END,FAIL                                               # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022

ml minimap2/2.26-GCCcore-12.2.0
minimap2 -t 6 -ax asm5 --eqx B73_v5_genome.fa A188_genome.fa > A188_aligned_to_B73.sam

ml SyRI/1.6.3
syri -c A188_aligned_to_B73.sam -r B73_v5_genome.fa -q A188_genome.fa -k -F S


###Apparently the chromosome IDs do not match, which is odd because the chromosomes appear to be labeled the same way in both files (chr1)
###There are also apparently unplaced scaffolds and contigs
###I think it may work if I get rid of the scaffolds that don't match between files (basically only keeping the chromosome data)
###I will try using head like they used in the documentation for SyRI to get rid everything after the chromosomes: https://schneebergerlab.github.io/syri/pipeline.html

grep -n c A188_genome.fa
#24992964:>chr10 
#26915829:>c01_001
head -26915828 A188_genome.fa > A188_genome_trim.fa

grep -n c B73_v5_genome.fa
#24742658:>chr10
#26648102:>scaf_21
head -26648101 B73_v5_genome.fa > B73_v5_genome_trim.fa

#!/bin/bash
#SBATCH --job-name=Zm_minimap_syri                                         # Job name
#SBATCH --partition=batch                                                  # Partition (queue) name
#SBATCH --ntasks=1                                                         # Single task job
#SBATCH --cpus-per-task=6                                                  # Number of cores per task
#SBATCH --mem=350gb                                                        # Total memory for job
#SBATCH --time=24:00:00                                                    # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_minimap_syri2.out     # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_minimap_syri2.err      # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                       # Where to send mail
#SBATCH --mail-type=END,FAIL                                               # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022

ml minimap2/2.26-GCCcore-12.2.0
minimap2 -t 6 -ax asm5 --eqx B73_v5_genome_trim.fa A188_genome_trim.fa > A188_aligned_to_B73_trim.sam

ml SyRI/1.6.3
syri -c A188_aligned_to_B73_trim.sam -r B73_v5_genome_trim.fa -q A188_genome_trim.fa -k -F S


###This seems to have worked! Now I will convert the vcf to a tsv file and edit it to be a SNP file ready for SNPsplit

#!/bin/bash
#SBATCH --job-name=Zm_vcf2tsvpy                                           # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=1                                                 # Number of cores per task
#SBATCH --mem=50gb                                                        # Total memory for job
#SBATCH --time=6:00:00                                                    # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_vcf2tsvpy.out        # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_vcf2tsvpy.err         # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=END,FAIL                                              # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
ml Miniconda3/23.5.2-0
source activate /home/jms53460/vcf2tsvpy
vcf2tsvpy --input_vcf syri.vcf --out_tsv Zm_vcf_table.tsv 
conda deactivate


###Selected columns from the vcf_table
awk '{print $3,$1,$2,$6,$4,$5}' Zm_vcf_table.tsv OFS="\t" > Zm_variants.tsv
grep -i "chr" Zm_variants.tsv > Zm_variants2.tsv

###Alter the table to match required SNP file format
ml R/4.3.2-foss-2022b
R
Zm_variants2 <- read.csv("/scratch/jms53460/Maize_SGT_2022/Zm_variants2.tsv", sep="")
Zm_SNPs = Zm_variants2[,(1:4)]
Zm_SNPs[,5] = paste(Zm_variants2[,5], "/", Zm_variants2[,6], sep = "")
colnames(Zm_SNPs) = c("ID", "Chr", "Position", "SNP value", "Ref/SNP")
Zm_SNPs$"SNP value" = 1
write.table(Zm_SNPs, file = 'Zm_variants.tab', col.names = TRUE, row.names = FALSE, sep = '\t', quote = FALSE)
q()

grep "SNP" Zm_variants.tab > Zm_SNPs.tab

ml R/4.3.2-foss-2022b
R
Zm_SNPs <- read.delim("/scratch/jms53460/Maize_SGT_2022/Zm_SNPs.tab")
write.table(Zm_SNPs, file = 'Zm_SNPs2.tab', col.names = TRUE, row.names = FALSE, sep = '\t', quote = FALSE)
q()

ml SAMtools/1.16.1-GCC-11.3.0
ml SNPsplit/0.6.0-GCC-11.3.0-Perl-5.34.1
mkdir B73_v5_genome_dir
cp B73_v5_genome.fa B73_v5_genome_dir
SNPsplit_genome_preparation --vcf_file syri.vcf --reference_genome B73_v5_genome_dir --list_strains
#There were no genomes available according to SNPsplit_genome_preparation, but I think that just looks at the vcf, which lacks a strain name this time. Since I will skip vcf filtering, I may be able to do this by assigning a strain name in the SNP directory

###Trying to use SNPsplit_genome_preparation to make N-masked genome
mkdir SNPs_B73_v5
cp Zm_SNPs.tab SNPs_B73_v5

#!/bin/bash
#SBATCH --job-name=Zm_SNPsplit_genome_prep                                # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=1                                                 # Number of cores per task
#SBATCH --mem=50gb                                                        # Total memory for job
#SBATCH --time=6:00:00                                                    # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_SNPsplit_gp.out      # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_SNPsplit_gp.err       # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=END,FAIL                                              # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
ml SAMtools/1.16.1-GCC-11.3.0
ml SNPsplit/0.6.0-GCC-11.3.0-Perl-5.34.1

SNPsplit_genome_preparation --vcf_file syri.vcf --reference_genome B73_v5_genome_dir --strain B73_v5 --skip_filtering


###There were 0 Ns introduced, just like what was happening before with tobacco. I'll switch to using maskfasta again

#!/bin/bash
#SBATCH --job-name=Zm_bedtools_maskfasta                                             # Job name
#SBATCH --partition=batch                                                            # Partition (queue) name
#SBATCH --ntasks=1                                                                   # Single task job
#SBATCH --cpus-per-task=1                                                            # Number of cores per task
#SBATCH --mem=50gb                                                                   # Total memory for job
#SBATCH --time=6:00:00                                                               # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_bedtools_maskfasta.out          # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_bedtools_maskfasta.err           # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                                 # Where to send mail
#SBATCH --mail-type=END,FAIL                                                         # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
ml BEDTools/2.30.0-GCC-12.2.0
bedtools maskfasta -fi B73_v5_genome.fa -fo Zm_N-masked_genome.fa -bed syri.vcf -fullHeader

###It seems to have worked. Now I need to get some of the maize data ready so I can try SNPsplit.


###Demultiplex the raw data

###Install fastq-multx
ml Miniconda3/23.5.2-0
conda create -p /scratch/jms53460/Fastq-Multx -c bioconda fastq-multx #when prompted, say 'y'
source activate /scratch/jms53460/Fastq-Multx/
fastq-multx
conda deactivate

cd /scratch/jms53460/Maize_SGT_2022
mkdir Raw_Data
mkdir Mapped_Data
mkdir Mapped_Data/demultiplexed
mv *fastq.gz Raw_Data
cp /home/jms53460/CELSeq_barcodes.txt /scratch/jms53460/Maize_SGT_2022/


#!/bin/bash
#SBATCH --job-name=Zm_demultiplex                                         # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=6                                                 # Number of cores per task
#SBATCH --mem=200gb                                                       # Total memory for job
#SBATCH --time=48:00:00                                                   # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_dm.out               # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_dm.err                # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=END,FAIL                                              # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
ml Miniconda3/23.5.2-0
source activate /scratch/jms53460/Fastq-Multx/

for file in Raw_Data/*_R1*.gz; do
    file2="${file:9:-12}"

    if [ ! -f "Mapped_Data/demultiplexed/""$file2""_1s.fastq.gz" ]; then
        module load fastp/0.23.2-GCC-11.3.0
	    fastp -w 6 -i "$file" -I "Raw_Data/""$file2""_R2.fastq.gz" -o "Mapped_Data/demultiplexed/umi_""$file2""_R1.fastq.gz" -O "Mapped_Data/demultiplexed/umi_""$file2""_R2.fastq.gz" -A -Q -L --umi --umi_loc read1 --umi_len 10 --umi_prefix UMI

	    fastq-multx -b -B "CELSeq_barcodes.txt" -m 0 "Mapped_Data/demultiplexed/umi_""$file2""_R1.fastq.gz" "Mapped_Data/demultiplexed/umi_""$file2""_R2.fastq.gz" -o "Mapped_Data/demultiplexed/""$file2""_%_R1.fastq.gz" "Mapped_Data/demultiplexed/""$file2""_%.fastq.gz"  # Split read 2 file by CELseq barcodes. Require perfect match to barcode in expected location

	    find "Mapped_Data/demultiplexed/" -name "umi_*" -delete
	    find "Mapped_Data/demultiplexed/" -name "*_R1*" -delete
    fi
done
conda deactivate


###after the fastp command with this file: Raw_Data/RPI1_1a_L2_R1.fastq.gz the error file says the sequence and quality have different length
mv Raw_Data/RPI2_1B-2_L3_* /scratch/jms53460/Maize_SGT_2022/
###The demultiplexing job seemed to be stuck so I moved the file it was stuck on and reran code for demultiplexing
###I realized the if statement had a typo and wasn't working, so I fixed it and stopped the job. I am deleting the files that may have been partially overwritten in Mapped_Data/demultiplexed and will then restart the job
rm Mapped_Data/demultiplexed/*PI10_1B-10_L3*

mv Raw_Data/RPI2_1a_L3* /scratch/jms53460/Maize_SGT_2022/
###Tried moving this one too and rerunning in case this was another problematic one

###It seems like there's something wrong with multiple files. Maybe I need to redownload them?


#!/bin/bash
#SBATCH --job-name=download_maize_data                                     # Job name
#SBATCH --partition=batch                                                  # Partition (queue) name
#SBATCH --ntasks=1                                                         # Single task job
#SBATCH --cpus-per-task=1                                                  # Number of cores per task
#SBATCH --mem=50gb                                                         # Total memory for job
#SBATCH --time=72:00:00                                                    # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_download3.out         # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_download3.err          # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                       # Where to send mail
#SBATCH --mail-type=END,FAIL                                               # Mail events (BEGIN, END, FAIL, ALL)

mkdir /scratch/jms53460/Maize_SGT_2022/Raw_Data2
cd /scratch/jms53460/Maize_SGT_2022/Raw_Data2
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL200149798-1a_H7MM2BBXX_L2_R1.fastq.gz > RPI1_1a_L2_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI1_CKDL200149798-1a_H7MM2BBXX_L2_R2.fastq.gz > RPI1_1a_L2_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL190144505-1B-2_HNLFWDSXX_L3_R1.fastq.gz > RPI2_1B-2_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL190144505-1B-2_HNLFWDSXX_L3_R2.fastq.gz > RPI2_1B-2_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL200149799-1a_H7MM2BBXX_L3_R1.fastq.gz > RPI2_1a_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI2_CKDL200149799-1a_H7MM2BBXX_L3_R2.fastq.gz > RPI2_1a_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI3_CKDL190144505-1B-3_HNLFWDSXX_L3_R1.fastq.gz > RPI3_1B-3_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI3_CKDL190144505-1B-3_HNLFWDSXX_L3_R2.fastq.gz > RPI3_1B-3_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI4_CKDL190144505-1B-4_HNLFWDSXX_L3_R1.fastq.gz > RPI4_1B-4_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI4_CKDL190144505-1B-4_HNLFWDSXX_L3_R2.fastq.gz > RPI4_1B-4_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI5_CKDL190144505-1B-5_HNLFWDSXX_L3_R1.fastq.gz > RPI5_1B-5_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI5_CKDL190144505-1B-5_HNLFWDSXX_L3_R2.fastq.gz > RPI5_1B-5_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI6_CKDL190144505-1B-6_HNLFWDSXX_L3_R1.fastq.gz > RPI6_1B-6_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI6_CKDL190144505-1B-6_HNLFWDSXX_L3_R2.fastq.gz > RPI6_1B-6_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI7_CKDL190144505-1B-7_HNLFWDSXX_L3_R1.fastq.gz > RPI7_1B-7_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI7_CKDL190144505-1B-7_HNLFWDSXX_L3_R2.fastq.gz > RPI7_1B-7_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI8_CKDL190144505-1B-8_HNLFWDSXX_L3_R1.fastq.gz > RPI8_1B-8_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI8_CKDL190144505-1B-8_HNLFWDSXX_L3_R2.fastq.gz > RPI8_1B-8_L3_R2.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI9_CKDL190144505-1B-9_HNLFWDSXX_L3_R1.fastq.gz > RPI9_1B-9_L3_R1.fastq.gz
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/RPI9_CKDL190144505-1B-9_HNLFWDSXX_L3_R2.fastq.gz > RPI9_1B-9_L3_R2.fastq.gz


rm Mapped_Data/demultiplexed/*PI1_1a_L2*
rm Mapped_Data/demultiplexed/*PI2_1a_L3*
rm Mapped_Data/demultiplexed/*PI3_1B-3_L3*

#!/bin/bash
#SBATCH --job-name=Zm_demultiplex                                         # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=6                                                 # Number of cores per task
#SBATCH --mem=200gb                                                       # Total memory for job
#SBATCH --time=48:00:00                                                   # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_dm2.out              # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_dm2.err               # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=END,FAIL                                              # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
ml Miniconda3/23.5.2-0
source activate /scratch/jms53460/Fastq-Multx/

for file in Raw_Data/*_R1*.gz; do
    file2="${file:9:-12}"

    if [ ! -f "Mapped_Data/demultiplexed/""$file2""_1s.fastq.gz" ]; then
        module load fastp/0.23.2-GCC-11.3.0
	    fastp -w 6 -i "$file" -I "Raw_Data2/""$file2""_R2.fastq.gz" -o "Mapped_Data/demultiplexed/umi_""$file2""_R1.fastq.gz" -O "Mapped_Data/demultiplexed/umi_""$file2""_R2.fastq.gz" -A -Q -L --umi --umi_loc read1 --umi_len 10 --umi_prefix UMI

	    fastq-multx -b -B "CELSeq_barcodes.txt" -m 0 "Mapped_Data/demultiplexed/umi_""$file2""_R1.fastq.gz" "Mapped_Data/demultiplexed/umi_""$file2""_R2.fastq.gz" -o "Mapped_Data/demultiplexed/""$file2""_%_R1.fastq.gz" "Mapped_Data/demultiplexed/""$file2""_%.fastq.gz"  # Split read 2 file by CELseq barcodes. Require perfect match to barcode in expected location

	    find "Mapped_Data/demultiplexed/" -name "umi_*" -delete
	    find "Mapped_Data/demultiplexed/" -name "*_R1*" -delete
    fi
done
conda deactivate


###There is a problem with some of these files, so I will move forward with some of the files that worked for now
mkdir Mapped_Data/demultiplexed2
cp Mapped_Data/demultiplexed/RPI10_1B-10_L3* Mapped_Data/demultiplexed2


###Make hisat2 index and run fastp

#!/bin/bash
#SBATCH --job-name=Zm_index_fastp                                         # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=6                                                 # Number of cores per task
#SBATCH --mem=50gb                                                       # Total memory for job
#SBATCH --time=12:00:00                                                   # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_index_fastp.out      # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_index_fastp.err       # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=END,FAIL                                              # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
ml HISAT2/3n-20201216-gompi-2022a
hisat2-build B73_v5_genome.fa B73_v5_hisat2_index
mkdir hisat2_out

for file in "Mapped_Data/demultiplexed2/"*.fastq*
do
	file2="${file:27:-9}"

if [ ! -f "hisat2_out/""$file2"".bam" ]; then

	module load fastp/0.23.2-GCC-11.3.0
	fastp -w 6 -i "$file" -o "hisat2_out/""$file2"".fastq.gz" -y -x -3 -a AAAAAAAAAAAA

fi
done


###I made the index for the wrong genome, I needed to index the N-masked genome. So I'll do that in the next script

###R code to make a list of hisat2_out/*fastq.gz files
ml R/4.3.2-foss-2022b
R
directory1 = 'hisat2_out'
f = dir(directory1)
runs = unique(f[grepl('fastq.gz', f)])
out = NULL
out = c(out, '', paste(c('hisat2 -p 6 --dta -x /scratch/jms53460/Maize_SGT_2022/Zm_N-masked_hisat2_index -U ', paste(paste(directory1, '/', runs, sep = ''), collapse=','), ' | samtools view -bS -> hisat2_out/merged_unsorted.bam'), collapse = ''))
write.csv(out, row.names=F, quote = F, file = 'generated_code.txt')
q()
head generated_code.txt

#Copied and pasted the code generated in R in the bash script below
#Map to genome (hisat2), .bam output (samtools view), sort (samtools sort), index (samtools index)

#!/bin/bash
#SBATCH --job-name=Zm_hisat2                                              # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=6                                                 # Number of cores per task
#SBATCH --mem=50gb                                                        # Total memory for job
#SBATCH --time=24:00:00                                                   # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_hs2.out              # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_hs2.err               # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=END,FAIL                                              # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
ml HISAT2/3n-20201216-gompi-2022a
ml SAMtools/1.16.1-GCC-11.3.0
hisat2-build Zm_N-masked_genome.fa Zm_N-masked_hisat2_index
hisat2 -p 6 --dta -x /scratch/jms53460/Maize_SGT_2022/Zm_N-masked_hisat2_index -U hisat2_out/RPI10_1B-10_L3_10s.fastq.gz,hisat2_out/RPI10_1B-10_L3_11s.fastq.gz,hisat2_out/RPI10_1B-10_L3_12s.fastq.gz,hisat2_out/RPI10_1B-10_L3_13s.fastq.gz,hisat2_out/RPI10_1B-10_L3_14s.fastq.gz,hisat2_out/RPI10_1B-10_L3_15s.fastq.gz,hisat2_out/RPI10_1B-10_L3_16s.fastq.gz,hisat2_out/RPI10_1B-10_L3_17s.fastq.gz,hisat2_out/RPI10_1B-10_L3_18s.fastq.gz,hisat2_out/RPI10_1B-10_L3_19s.fastq.gz,hisat2_out/RPI10_1B-10_L3_1s.fastq.gz,hisat2_out/RPI10_1B-10_L3_20s.fastq.gz,hisat2_out/RPI10_1B-10_L3_21s.fastq.gz,hisat2_out/RPI10_1B-10_L3_22s.fastq.gz,hisat2_out/RPI10_1B-10_L3_23s.fastq.gz,hisat2_out/RPI10_1B-10_L3_24s.fastq.gz,hisat2_out/RPI10_1B-10_L3_25s.fastq.gz,hisat2_out/RPI10_1B-10_L3_26s.fastq.gz,hisat2_out/RPI10_1B-10_L3_27s.fastq.gz,hisat2_out/RPI10_1B-10_L3_28s.fastq.gz,hisat2_out/RPI10_1B-10_L3_29s.fastq.gz,hisat2_out/RPI10_1B-10_L3_2s.fastq.gz,hisat2_out/RPI10_1B-10_L3_30s.fastq.gz,hisat2_out/RPI10_1B-10_L3_31s.fastq.gz,hisat2_out/RPI10_1B-10_L3_32s.fastq.gz,hisat2_out/RPI10_1B-10_L3_33s.fastq.gz,hisat2_out/RPI10_1B-10_L3_34s.fastq.gz,hisat2_out/RPI10_1B-10_L3_35s.fastq.gz,hisat2_out/RPI10_1B-10_L3_36s.fastq.gz,hisat2_out/RPI10_1B-10_L3_37s.fastq.gz,hisat2_out/RPI10_1B-10_L3_38s.fastq.gz,hisat2_out/RPI10_1B-10_L3_39s.fastq.gz,hisat2_out/RPI10_1B-10_L3_3s.fastq.gz,hisat2_out/RPI10_1B-10_L3_40s.fastq.gz,hisat2_out/RPI10_1B-10_L3_41s.fastq.gz,hisat2_out/RPI10_1B-10_L3_42s.fastq.gz,hisat2_out/RPI10_1B-10_L3_43s.fastq.gz,hisat2_out/RPI10_1B-10_L3_44s.fastq.gz,hisat2_out/RPI10_1B-10_L3_45s.fastq.gz,hisat2_out/RPI10_1B-10_L3_46s.fastq.gz,hisat2_out/RPI10_1B-10_L3_47s.fastq.gz,hisat2_out/RPI10_1B-10_L3_48s.fastq.gz,hisat2_out/RPI10_1B-10_L3_49s.fastq.gz,hisat2_out/RPI10_1B-10_L3_4s.fastq.gz,hisat2_out/RPI10_1B-10_L3_50s.fastq.gz,hisat2_out/RPI10_1B-10_L3_51s.fastq.gz,hisat2_out/RPI10_1B-10_L3_52s.fastq.gz,hisat2_out/RPI10_1B-10_L3_53s.fastq.gz,hisat2_out/RPI10_1B-10_L3_54s.fastq.gz,hisat2_out/RPI10_1B-10_L3_55s.fastq.gz,hisat2_out/RPI10_1B-10_L3_56s.fastq.gz,hisat2_out/RPI10_1B-10_L3_57s.fastq.gz,hisat2_out/RPI10_1B-10_L3_58s.fastq.gz,hisat2_out/RPI10_1B-10_L3_59s.fastq.gz,hisat2_out/RPI10_1B-10_L3_5s.fastq.gz,hisat2_out/RPI10_1B-10_L3_60s.fastq.gz,hisat2_out/RPI10_1B-10_L3_61s.fastq.gz,hisat2_out/RPI10_1B-10_L3_62s.fastq.gz,hisat2_out/RPI10_1B-10_L3_63s.fastq.gz,hisat2_out/RPI10_1B-10_L3_64s.fastq.gz,hisat2_out/RPI10_1B-10_L3_65s.fastq.gz,hisat2_out/RPI10_1B-10_L3_66s.fastq.gz,hisat2_out/RPI10_1B-10_L3_67s.fastq.gz,hisat2_out/RPI10_1B-10_L3_68s.fastq.gz,hisat2_out/RPI10_1B-10_L3_69s.fastq.gz,hisat2_out/RPI10_1B-10_L3_6s.fastq.gz,hisat2_out/RPI10_1B-10_L3_70s.fastq.gz,hisat2_out/RPI10_1B-10_L3_71s.fastq.gz,hisat2_out/RPI10_1B-10_L3_72s.fastq.gz,hisat2_out/RPI10_1B-10_L3_73s.fastq.gz,hisat2_out/RPI10_1B-10_L3_74s.fastq.gz,hisat2_out/RPI10_1B-10_L3_75s.fastq.gz,hisat2_out/RPI10_1B-10_L3_76s.fastq.gz,hisat2_out/RPI10_1B-10_L3_77s.fastq.gz,hisat2_out/RPI10_1B-10_L3_78s.fastq.gz,hisat2_out/RPI10_1B-10_L3_79s.fastq.gz,hisat2_out/RPI10_1B-10_L3_7s.fastq.gz,hisat2_out/RPI10_1B-10_L3_80s.fastq.gz,hisat2_out/RPI10_1B-10_L3_81s.fastq.gz,hisat2_out/RPI10_1B-10_L3_82s.fastq.gz,hisat2_out/RPI10_1B-10_L3_83s.fastq.gz,hisat2_out/RPI10_1B-10_L3_84s.fastq.gz,hisat2_out/RPI10_1B-10_L3_85s.fastq.gz,hisat2_out/RPI10_1B-10_L3_86s.fastq.gz,hisat2_out/RPI10_1B-10_L3_87s.fastq.gz,hisat2_out/RPI10_1B-10_L3_88s.fastq.gz,hisat2_out/RPI10_1B-10_L3_89s.fastq.gz,hisat2_out/RPI10_1B-10_L3_8s.fastq.gz,hisat2_out/RPI10_1B-10_L3_90s.fastq.gz,hisat2_out/RPI10_1B-10_L3_91s.fastq.gz,hisat2_out/RPI10_1B-10_L3_92s.fastq.gz,hisat2_out/RPI10_1B-10_L3_93s.fastq.gz,hisat2_out/RPI10_1B-10_L3_94s.fastq.gz,hisat2_out/RPI10_1B-10_L3_95s.fastq.gz,hisat2_out/RPI10_1B-10_L3_96s.fastq.gz,hisat2_out/RPI10_1B-10_L3_9s.fastq.gz,hisat2_out/RPI10_1B-10_L3_unmatched.fastq.gz | samtools view -bS -> hisat2_out/merged_unsorted.bam
samtools sort -@ 6 hisat2_out/merged_unsorted.bam -o hisat2_out/merged_s.bam
samtools index -@ 6 hisat2_out/merged_s.bam


###Tring SNPsplit

#!/bin/bash
#SBATCH --job-name=Zm_SNPsplit                                            # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=1                                                 # Number of cores per task
#SBATCH --mem=50gb                                                        # Total memory for job
#SBATCH --time=12:00:00                                                   # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_SNPsplit.out         # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_SNPsplit.err          # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=END,FAIL                                              # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
ml SAMtools/1.16.1-GCC-11.3.0
ml SNPsplit/0.6.0-GCC-11.3.0-Perl-5.34.1

SNPsplit --conflicting -o Zm_SNPsplit --snp_file Zm_SNPs.tab hisat2_out/merged_s.bam


###It seems to have worked. The proportions of reads specific for each genome are low compared to what Brad recalls observing before, so there is still some optimization needed, but overall I consider this goal met.

#Allele-specific single-end sorting report
#=========================================
#Read alignments processed in total:             324801816
#Reads were unassignable:                        318237590 (97.98%)
#Reads were specific for genome 1:               3016984 (0.93%)
#Reads were specific for genome 2:               2558188 (0.79%)
#Reads contained conflicting SNP information:    989054 (0.30)


mkdir Mapped_Data/demultiplexed3
cp Mapped_Data/demultiplexed/RPI1_1a_L2_* Mapped_Data/demultiplexed3


#!/bin/bash
#SBATCH --job-name=Zm_download_v4                                         # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=1                                                 # Number of cores per task
#SBATCH --mem=50gb                                                        # Total memory for job
#SBATCH --time=3:00:00                                                    # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_download4.out         # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_download4.err          # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=END,FAIL                                              # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022

curl -s https://stacks.stanford.edu/file/druid:js115sm3463/4o-A188_B73.AB.final.vcf > v4_A188_B73.vcf
curl -s https://stacks.stanford.edu/file/druid:js115sm3463/A188_SNPs.tab.gz > v4_SNPs.tab.gz
curl -s https://download.maizegdb.org/Zm-B73-REFERENCE-GRAMENE-4.0/Zm-B73-REFERENCE-GRAMENE-4.0.fa.gz > v4_B73.fa


gzip -d v4_SNPs.tab.gz

###The samples I used were actually not samples from the 2022 SGT paper, and were likely from W22 plants, so it makes sense that this didn't go quite as hoped. I'll retry using some of the correct samples
###The samples used RPI1 through RPI6 (labeled Lane1), plus doubles of RPI1 (Lane2_RPI1, RPI1b in Illumina barcode column) and RPI2 (Lane3_RPI2, RPI2b in Illumnina barcode column).


###Running pipeline on RPI1_1a_L2 samples

###Run fastp

#!/bin/bash
#SBATCH --job-name=Zm_fastp2                                              # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=6                                                 # Number of cores per task
#SBATCH --mem=50gb                                                        # Total memory for job
#SBATCH --time=12:00:00                                                   # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_fastp2.out           # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_fastp2.err            # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=END,FAIL                                              # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
mkdir hisat2_out2

for file in "Mapped_Data/demultiplexed3/"*.fastq*
do
	file2="${file:27:-9}"

if [ ! -f "hisat2_out2/""$file2"".bam" ]; then

	module load fastp/0.23.2-GCC-11.3.0
	fastp -w 6 -i "$file" -o "hisat2_out2/""$file2"".fastq.gz" -y -x -3 -a AAAAAAAAAAAA

fi
done


###R code to make a list of hisat2_out2/*fastq.gz files
ml R/4.3.2-foss-2022b
R
directory1 = 'hisat2_out2'
f = dir(directory1)
runs = unique(f[grepl('fastq.gz', f)])
out = NULL
out = c(out, '', paste(c('hisat2 -p 6 --dta -x /scratch/jms53460/Maize_SGT_2022/Zm_N-masked_hisat2_index -U ', paste(paste(directory1, '/', runs, sep = ''), collapse=','), ' | samtools view -bS -> hisat2_out2/merged_unsorted.bam'), collapse = ''))
write.csv(out, row.names=F, quote = F, file = 'generated_code2.txt')
q()
head generated_code2.txt

#Copied and pasted the code generated in R in the bash script below
#Map to genome (hisat2), .bam output (samtools view), sort (samtools sort), index (samtools index)

#!/bin/bash
#SBATCH --job-name=Zm_hisat2_2                                            # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=6                                                 # Number of cores per task
#SBATCH --mem=50gb                                                        # Total memory for job
#SBATCH --time=24:00:00                                                   # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_hs2_2.out            # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_hs2_2.err             # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=END,FAIL                                              # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
ml HISAT2/3n-20201216-gompi-2022a
ml SAMtools/1.16.1-GCC-11.3.0
hisat2 -p 6 --dta -x /scratch/jms53460/Maize_SGT_2022/Zm_N-masked_hisat2_index -U hisat2_out2/RPI1_1a_L2_10s.fastq.gz,hisat2_out2/RPI1_1a_L2_11s.fastq.gz,hisat2_out2/RPI1_1a_L2_12s.fastq.gz,hisat2_out2/RPI1_1a_L2_13s.fastq.gz,hisat2_out2/RPI1_1a_L2_14s.fastq.gz,hisat2_out2/RPI1_1a_L2_15s.fastq.gz,hisat2_out2/RPI1_1a_L2_16s.fastq.gz,hisat2_out2/RPI1_1a_L2_17s.fastq.gz,hisat2_out2/RPI1_1a_L2_18s.fastq.gz,hisat2_out2/RPI1_1a_L2_19s.fastq.gz,hisat2_out2/RPI1_1a_L2_1s.fastq.gz,hisat2_out2/RPI1_1a_L2_20s.fastq.gz,hisat2_out2/RPI1_1a_L2_21s.fastq.gz,hisat2_out2/RPI1_1a_L2_22s.fastq.gz,hisat2_out2/RPI1_1a_L2_23s.fastq.gz,hisat2_out2/RPI1_1a_L2_24s.fastq.gz,hisat2_out2/RPI1_1a_L2_25s.fastq.gz,hisat2_out2/RPI1_1a_L2_26s.fastq.gz,hisat2_out2/RPI1_1a_L2_27s.fastq.gz,hisat2_out2/RPI1_1a_L2_28s.fastq.gz,hisat2_out2/RPI1_1a_L2_29s.fastq.gz,hisat2_out2/RPI1_1a_L2_2s.fastq.gz,hisat2_out2/RPI1_1a_L2_30s.fastq.gz,hisat2_out2/RPI1_1a_L2_31s.fastq.gz,hisat2_out2/RPI1_1a_L2_32s.fastq.gz,hisat2_out2/RPI1_1a_L2_33s.fastq.gz,hisat2_out2/RPI1_1a_L2_34s.fastq.gz,hisat2_out2/RPI1_1a_L2_35s.fastq.gz,hisat2_out2/RPI1_1a_L2_36s.fastq.gz,hisat2_out2/RPI1_1a_L2_37s.fastq.gz,hisat2_out2/RPI1_1a_L2_38s.fastq.gz,hisat2_out2/RPI1_1a_L2_39s.fastq.gz,hisat2_out2/RPI1_1a_L2_3s.fastq.gz,hisat2_out2/RPI1_1a_L2_40s.fastq.gz,hisat2_out2/RPI1_1a_L2_41s.fastq.gz,hisat2_out2/RPI1_1a_L2_42s.fastq.gz,hisat2_out2/RPI1_1a_L2_43s.fastq.gz,hisat2_out2/RPI1_1a_L2_44s.fastq.gz,hisat2_out2/RPI1_1a_L2_45s.fastq.gz,hisat2_out2/RPI1_1a_L2_46s.fastq.gz,hisat2_out2/RPI1_1a_L2_47s.fastq.gz,hisat2_out2/RPI1_1a_L2_48s.fastq.gz,hisat2_out2/RPI1_1a_L2_49s.fastq.gz,hisat2_out2/RPI1_1a_L2_4s.fastq.gz,hisat2_out2/RPI1_1a_L2_50s.fastq.gz,hisat2_out2/RPI1_1a_L2_51s.fastq.gz,hisat2_out2/RPI1_1a_L2_52s.fastq.gz,hisat2_out2/RPI1_1a_L2_53s.fastq.gz,hisat2_out2/RPI1_1a_L2_54s.fastq.gz,hisat2_out2/RPI1_1a_L2_55s.fastq.gz,hisat2_out2/RPI1_1a_L2_56s.fastq.gz,hisat2_out2/RPI1_1a_L2_57s.fastq.gz,hisat2_out2/RPI1_1a_L2_58s.fastq.gz,hisat2_out2/RPI1_1a_L2_59s.fastq.gz,hisat2_out2/RPI1_1a_L2_5s.fastq.gz,hisat2_out2/RPI1_1a_L2_60s.fastq.gz,hisat2_out2/RPI1_1a_L2_61s.fastq.gz,hisat2_out2/RPI1_1a_L2_62s.fastq.gz,hisat2_out2/RPI1_1a_L2_63s.fastq.gz,hisat2_out2/RPI1_1a_L2_64s.fastq.gz,hisat2_out2/RPI1_1a_L2_65s.fastq.gz,hisat2_out2/RPI1_1a_L2_66s.fastq.gz,hisat2_out2/RPI1_1a_L2_67s.fastq.gz,hisat2_out2/RPI1_1a_L2_68s.fastq.gz,hisat2_out2/RPI1_1a_L2_69s.fastq.gz,hisat2_out2/RPI1_1a_L2_6s.fastq.gz,hisat2_out2/RPI1_1a_L2_70s.fastq.gz,hisat2_out2/RPI1_1a_L2_71s.fastq.gz,hisat2_out2/RPI1_1a_L2_72s.fastq.gz,hisat2_out2/RPI1_1a_L2_73s.fastq.gz,hisat2_out2/RPI1_1a_L2_74s.fastq.gz,hisat2_out2/RPI1_1a_L2_75s.fastq.gz,hisat2_out2/RPI1_1a_L2_76s.fastq.gz,hisat2_out2/RPI1_1a_L2_77s.fastq.gz,hisat2_out2/RPI1_1a_L2_78s.fastq.gz,hisat2_out2/RPI1_1a_L2_79s.fastq.gz,hisat2_out2/RPI1_1a_L2_7s.fastq.gz,hisat2_out2/RPI1_1a_L2_80s.fastq.gz,hisat2_out2/RPI1_1a_L2_81s.fastq.gz,hisat2_out2/RPI1_1a_L2_82s.fastq.gz,hisat2_out2/RPI1_1a_L2_83s.fastq.gz,hisat2_out2/RPI1_1a_L2_84s.fastq.gz,hisat2_out2/RPI1_1a_L2_85s.fastq.gz,hisat2_out2/RPI1_1a_L2_86s.fastq.gz,hisat2_out2/RPI1_1a_L2_87s.fastq.gz,hisat2_out2/RPI1_1a_L2_88s.fastq.gz,hisat2_out2/RPI1_1a_L2_89s.fastq.gz,hisat2_out2/RPI1_1a_L2_8s.fastq.gz,hisat2_out2/RPI1_1a_L2_90s.fastq.gz,hisat2_out2/RPI1_1a_L2_91s.fastq.gz,hisat2_out2/RPI1_1a_L2_92s.fastq.gz,hisat2_out2/RPI1_1a_L2_93s.fastq.gz,hisat2_out2/RPI1_1a_L2_94s.fastq.gz,hisat2_out2/RPI1_1a_L2_95s.fastq.gz,hisat2_out2/RPI1_1a_L2_96s.fastq.gz,hisat2_out2/RPI1_1a_L2_9s.fastq.gz,hisat2_out2/RPI1_1a_L2_unmatched.fastq.gz | samtools view -bS -> hisat2_out2/merged_unsorted.bam
samtools sort -@ 6 hisat2_out2/merged_unsorted.bam -o hisat2_out2/merged_s.bam
samtools index -@ 6 hisat2_out2/merged_s.bam


###Tring SNPsplit

#!/bin/bash
#SBATCH --job-name=Zm_SNPsplit2                                           # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=1                                                 # Number of cores per task
#SBATCH --mem=50gb                                                        # Total memory for job
#SBATCH --time=12:00:00                                                   # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_SNPsplit2.out        # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_SNPsplit2.err         # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=BEGIN,END,FAIL                                        # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
ml SAMtools/1.16.1-GCC-11.3.0
ml SNPsplit/0.6.0-GCC-11.3.0-Perl-5.34.1

SNPsplit --conflicting -o Zm_SNPsplit2 --snp_file Zm_SNPs.tab hisat2_out2/merged_s.bam


wc -l hisat2_out2/merged_s.bam #140488880
#Somehow SNPsplit is processing a lot more lines than exist in my bam file? Already at 444000000 lines processed
wc -l hisat2_out/merged_s.bam #46302542 lines in the previous run
#SNPsplit processed 350084986 read alignments in the previous run
#Based on this ratio SNPsplit should read ~1062210528 read alignments this time

###There were still not that many reads matching the two genomes (about 2% genome 1 and 1.5% genome 2). I suspect there may be an issue with the N-masking including non-SNP regions. It looks like SNPsplit_genome_preparation expects the SNP file to be split by chromosome, so I can try splitting the file up and running that again. I may also want an alternative method to make a vcf file where I don't have to exclude scaffolds.
###I'm trying to make a vcf file using the alignment from minimap2 and bcftools mpileup



#!/bin/bash
#SBATCH --job-name=Zm_mpileup                                             # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=12                                                # Number of cores per task
#SBATCH --mem=100gb                                                        # Total memory for job
#SBATCH --time=24:00:00                                                   # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_mpileup.out          # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_mpileup.err           # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=END,FAIL                                              # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
module load SAMtools/1.16.1-GCC-11.3.0
samtools view -@ 12 -bS A188_aligned_to_B73.sam > A188_aligned_to_B73.bam
samtools index -@ 12 A188_aligned_to_B73.bam

module load BCFtools/1.15.1-GCC-11.3.0
bcftools mpileup -Ou --threads 12 -d 10000000000000 --min-MQ 60 -f B73_v5_genome.fa A188_aligned_to_B73.bam | bcftools call -Ou -m -v --threads 12 | bcftools filter --threads 12 -Oz -e 'QUAL<40 || DP<10' > Zm_mpileup.vcf.gz
bcftools index --threads 12 Zm_mpileup.vcf.gz


###This is not working. The sam file is not coverting to a bam file successfully.


###Trying to use SNPsplit_genome_preparation after splitting SNP file by chromosomes
grep chr1 SNPs_B73_v5/Zm_SNPs.tab > SNPs_B73_v5/chr1.txt
grep chr2 SNPs_B73_v5/Zm_SNPs.tab > SNPs_B73_v5/chr2.txt
grep chr3 SNPs_B73_v5/Zm_SNPs.tab > SNPs_B73_v5/chr3.txt
grep chr4 SNPs_B73_v5/Zm_SNPs.tab > SNPs_B73_v5/chr4.txt
grep chr5 SNPs_B73_v5/Zm_SNPs.tab > SNPs_B73_v5/chr5.txt
grep chr6 SNPs_B73_v5/Zm_SNPs.tab > SNPs_B73_v5/chr6.txt
grep chr7 SNPs_B73_v5/Zm_SNPs.tab > SNPs_B73_v5/chr7.txt
grep chr8 SNPs_B73_v5/Zm_SNPs.tab > SNPs_B73_v5/chr8.txt
grep chr9 SNPs_B73_v5/Zm_SNPs.tab > SNPs_B73_v5/chr9.txt
grep chr10 SNPs_B73_v5/Zm_SNPs.tab > SNPs_B73_v5/chr10.txt

#!/bin/bash
#SBATCH --job-name=Zm_SNPsplit_genome_prep2                               # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=1                                                 # Number of cores per task
#SBATCH --mem=50gb                                                        # Total memory for job
#SBATCH --time=6:00:00                                                    # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_SNPsplit_gp2.out     # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_SNPsplit_gp2.err      # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=END,FAIL                                              # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
ml SAMtools/1.16.1-GCC-11.3.0
ml SNPsplit/0.6.0-GCC-11.3.0-Perl-5.34.1

SNPsplit_genome_preparation --vcf_file syri.vcf --reference_genome B73_v5_genome_dir --strain B73_v5 --skip_filtering



###Next I'll try using v4 files and see if things go differently

mkdir v4_B73_genome_dir
cp v4_B73.fa v4_B73_genome_dir
mkdir SNPs_v4_B73
cp v4_SNPs.tab SNPs_v4_B73

#!/bin/bash
#SBATCH --job-name=Zm_SNPsplit_genome_prep_v4                             # Job name
#SBATCH --partition=batch                                                 # Partition (queue) name
#SBATCH --ntasks=1                                                        # Single task job
#SBATCH --cpus-per-task=1                                                 # Number of cores per task
#SBATCH --mem=50gb                                                        # Total memory for job
#SBATCH --time=6:00:00                                                    # Time limit hrs:min:sec
#SBATCH --output=/scratch/jms53460/Maize_SGT_2022/Zm_SNPsplit_gp_v4.out   # Location of standard output file
#SBATCH --error=/scratch/jms53460/Maize_SGT_2022/Zm_SNPsplit_gp_v4.err    # Location of error log file
#SBATCH --mail-user=jms53460@uga.edu                                      # Where to send mail
#SBATCH --mail-type=END,FAIL                                              # Mail events (BEGIN, END, FAIL, ALL)

cd /scratch/jms53460/Maize_SGT_2022
ml SAMtools/1.16.1-GCC-11.3.0
ml SNPsplit/0.6.0-GCC-11.3.0-Perl-5.34.1

SNPsplit_genome_preparation --vcf_file v4_A188_B73.vcf --reference_genome v4_B73_genome_dir --strain v4_B73 --skip_filtering




###Next steps are to figure out what is going wrong with the files that are not making it through demultiplexing, remap the data using all the files, and run stringtie, featurecounts, UMIcounts, and R code to output data tables for visualizing the data in R 